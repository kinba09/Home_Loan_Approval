{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"text-align: center;\">\n  <h1>Home Loan Approval EDA and Prediction</h1>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"* This EDA and prediction project aims to explore the factors that influence home loan approval and to develop a model that can predict the likelihood of approval for a given borrower. \n* The project uses a dataset of home loan approval to train a machine learning model. \n* Once the model is trained, it can be used to predict the likelihood of approval for new loan applications.\n<br> </br>\n## Here's a brief explanation of each column:\n\n1. Loan_ID: A unique identifier for each loan application.\n\n1. Gender: The gender of the loan applicant (e.g., Male, Female).\n\n1. Married: Marital status of the applicant (e.g., Yes, No).\n\n1. Dependents: The number of dependents of the applicant (e.g., 0, 1, 2, 3+).\n\n1. Education: The educational background of the applicant (e.g., Graduate, Not Graduate).\n\n1. Self_Employed: Indicates whether the applicant is self-employed (e.g., Yes, No).\n\n1. ApplicantIncome: The income of the primary applicant.\n\n1. CoapplicantIncome: The income of the co-applicant (if any).\n\n1. LoanAmount: The loan amount requested by the applicant.\n\n1. Loan_Amount_Term: The term or duration of the loan in months.\n\n1. Credit_History: A binary variable indicating the credit history of the applicant (e.g., 1 for good credit history, 0 for bad credit history).\n\n1. Property_Area: The area or location of the property for which the loan is requested (e.g., Urban, Rural, Semiurban).\n\n1. Loan_Status: The target variable indicating whether the loan was approved or not (e.g., Y for Yes, N for No).","metadata":{}},{"cell_type":"markdown","source":"Github : https://github.com/kinba09/Home_Loan_Approval | Streamlit : will update | API : will update \n","metadata":{}},{"cell_type":"markdown","source":"# Importing the required libraries","metadata":{}},{"cell_type":"code","source":"pip install scikit-learn==1.3.0","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-09-24T09:55:19.762251Z","iopub.execute_input":"2023-09-24T09:55:19.763197Z","iopub.status.idle":"2023-09-24T09:55:37.055814Z","shell.execute_reply.started":"2023-09-24T09:55:19.763156Z","shell.execute_reply":"2023-09-24T09:55:37.054339Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nimport seaborn as sns\nimport sklearn\n\nplt.style.use('seaborn-v0_8-pastel')\npd.set_option('display.max_columns', 200)\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import RandomOverSampler\n\nimport warnings \nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:37.059986Z","iopub.execute_input":"2023-09-24T09:55:37.060340Z","iopub.status.idle":"2023-09-24T09:55:38.526534Z","shell.execute_reply.started":"2023-09-24T09:55:37.060307Z","shell.execute_reply":"2023-09-24T09:55:38.525449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(sklearn.__version__)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:38.528057Z","iopub.execute_input":"2023-09-24T09:55:38.528716Z","iopub.status.idle":"2023-09-24T09:55:38.534871Z","shell.execute_reply.started":"2023-09-24T09:55:38.528670Z","shell.execute_reply":"2023-09-24T09:55:38.533931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Setting Matplotlib Style:</h3>\n\n* `plt.style.use('seaborn-v0_8-pastel')`: Sets the Matplotlib style to 'seaborn-v0_8-pastel', which is a predefined style for creating pastel-colored plots with seaborn-like aesthetics.\n\n<h3>pandas Configuration:</h3>\n\n* `pd.set_option('display.max_columns', 200)`: Configures pandas to display up to 200 columns when you print DataFrames. This setting ensures that you can see a large number of columns in your DataFrames when needed.\n\n<h3>Importing Specific Functions:</h3>\n\n* `from sklearn.model_selection import train_test_split`: Imports the train_test_split function from scikit-learn, which is used for splitting a dataset into training and testing sets.\n\n* `from imblearn.over_sampling import RandomOverSampler`: Imports the RandomOverSampler class from the imbalanced-learn library, which is used for oversampling to balance imbalanced datasets in machine learning.\n\n<h3>Suppressing Warnings:</h3>\n\n* `warnings.filterwarnings('ignore')`: Temporarily suppresses warnings to avoid cluttering the output with warning messages.","metadata":{}},{"cell_type":"markdown","source":"# Importing and Reading the data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/home-loan-approval/loan_sanction_train.csv')\n#df = pd.read_csv('your_file.csv')","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:38.537403Z","iopub.execute_input":"2023-09-24T09:55:38.537741Z","iopub.status.idle":"2023-09-24T09:55:38.577655Z","shell.execute_reply.started":"2023-09-24T09:55:38.537714Z","shell.execute_reply":"2023-09-24T09:55:38.576811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:38.579218Z","iopub.execute_input":"2023-09-24T09:55:38.580183Z","iopub.status.idle":"2023-09-24T09:55:38.589090Z","shell.execute_reply.started":"2023-09-24T09:55:38.580142Z","shell.execute_reply":"2023-09-24T09:55:38.587947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:38.590572Z","iopub.execute_input":"2023-09-24T09:55:38.591025Z","iopub.status.idle":"2023-09-24T09:55:38.627913Z","shell.execute_reply.started":"2023-09-24T09:55:38.590985Z","shell.execute_reply":"2023-09-24T09:55:38.627120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:38.629027Z","iopub.execute_input":"2023-09-24T09:55:38.629662Z","iopub.status.idle":"2023-09-24T09:55:38.636361Z","shell.execute_reply.started":"2023-09-24T09:55:38.629606Z","shell.execute_reply":"2023-09-24T09:55:38.635279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:38.637961Z","iopub.execute_input":"2023-09-24T09:55:38.638311Z","iopub.status.idle":"2023-09-24T09:55:38.653319Z","shell.execute_reply.started":"2023-09-24T09:55:38.638280Z","shell.execute_reply":"2023-09-24T09:55:38.651958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:38.657388Z","iopub.execute_input":"2023-09-24T09:55:38.657736Z","iopub.status.idle":"2023-09-24T09:55:38.689947Z","shell.execute_reply.started":"2023-09-24T09:55:38.657707Z","shell.execute_reply":"2023-09-24T09:55:38.688534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`df.shape`: This line of code returns the shape of the DataFrame.\n\n`df.head(10)`: This code displays the first 10 rows of the DataFrame. It is useful for quickly inspecting the beginning of the dataset to get an overview of its contents.\n\n`df.columns`: This line of code retrieves the column names (or column labels) of the DataFrame df. \n\n`df.dtypes`: This code returns the data types of each column in the DataFrame df. It provides information about whether each column contains integers, floats, strings, or other data types.\n\n`df.describe()`: This line of code generates summary statistics for the numerical columns in the DataFrame df. It provides information such as the count, mean, standard deviation, minimum, and maximum values for each numerical column.","metadata":{}},{"cell_type":"markdown","source":"# Data Preperation","metadata":{}},{"cell_type":"code","source":"df = df[[#'Loan_ID', \n        'Gender', 'Married', 'Dependents', 'Education',\n       'Self_Employed', 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',\n       'Loan_Amount_Term', 'Credit_History', 'Property_Area', 'Loan_Status']].copy()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:38.693627Z","iopub.execute_input":"2023-09-24T09:55:38.693997Z","iopub.status.idle":"2023-09-24T09:55:38.708242Z","shell.execute_reply.started":"2023-09-24T09:55:38.693964Z","shell.execute_reply":"2023-09-24T09:55:38.706960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `df[['Gender', 'Married', ... 'Loan_Status']]` selects a subset of columns from the DataFrame `df`. It retains only the columns listed within the double square brackets.\n\n- `.copy()` creates a new DataFrame that is a deep copy of the selected columns. This is often done to ensure that modifications to the new DataFrame do not affect the original DataFrame `df`.\n","metadata":{}},{"cell_type":"markdown","source":"### Checking for null values","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:38.709554Z","iopub.execute_input":"2023-09-24T09:55:38.710228Z","iopub.status.idle":"2023-09-24T09:55:38.719595Z","shell.execute_reply.started":"2023-09-24T09:55:38.710194Z","shell.execute_reply":"2023-09-24T09:55:38.718314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(df.isnull().sum()/(len(df)))*100","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:38.720960Z","iopub.execute_input":"2023-09-24T09:55:38.722067Z","iopub.status.idle":"2023-09-24T09:55:38.733970Z","shell.execute_reply.started":"2023-09-24T09:55:38.722033Z","shell.execute_reply":"2023-09-24T09:55:38.732866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.dropna(axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:38.735676Z","iopub.execute_input":"2023-09-24T09:55:38.736105Z","iopub.status.idle":"2023-09-24T09:55:38.743803Z","shell.execute_reply.started":"2023-09-24T09:55:38.736063Z","shell.execute_reply":"2023-09-24T09:55:38.742825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:38.745084Z","iopub.execute_input":"2023-09-24T09:55:38.746310Z","iopub.status.idle":"2023-09-24T09:55:38.764268Z","shell.execute_reply.started":"2023-09-24T09:55:38.746267Z","shell.execute_reply":"2023-09-24T09:55:38.763109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `df.isna().sum()`: This code calculates the number of missing values (NaN) for each column in the DataFrame `df`. It returns a Series with the count of missing values for each column.\n\n- `(df.isnull().sum() / len(df)) * 100`: This code calculates the percentage of missing values for each column in the DataFrame `df`. It divides the count of missing values by the total number of rows (`len(df)`) and multiplies by 100 to get the percentage.\n\n- `df = df.dropna(axis=0)`: This code removes rows with missing values (NaN) from the DataFrame `df`. The `axis=0` argument specifies that rows containing any missing values should be dropped.\n\n- `df.isna().sum()`: After removing rows with missing values, this code calculates the updated number of missing values for each column in the DataFrame `df`. It returns a Series with the count of missing values for each column.\n","metadata":{}},{"cell_type":"code","source":"df.loc[df.duplicated()]","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:38.766209Z","iopub.execute_input":"2023-09-24T09:55:38.766695Z","iopub.status.idle":"2023-09-24T09:55:38.790500Z","shell.execute_reply.started":"2023-09-24T09:55:38.766603Z","shell.execute_reply":"2023-09-24T09:55:38.789720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df.reset_index()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:38.792659Z","iopub.execute_input":"2023-09-24T09:55:38.793685Z","iopub.status.idle":"2023-09-24T09:55:38.799038Z","shell.execute_reply.started":"2023-09-24T09:55:38.793640Z","shell.execute_reply":"2023-09-24T09:55:38.797962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `df.loc[df.duplicated()]`: This code identifies and returns rows in the DataFrame `df` that are duplicates. It checks for duplicated rows based on all columns by default. In this dataset, there arn't any duplicates.\n\n- `df = df.reset_index()`: This code resets the index of the DataFrame `df`. When resetting the index, a new default integer index is assigned to the DataFrame, and the old index is moved to a new column. This operation can be useful when you want to reorganize the index of your DataFrame.\n\n","metadata":{}},{"cell_type":"code","source":"df.head(10)  #to check the reseted index","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:38.800394Z","iopub.execute_input":"2023-09-24T09:55:38.801461Z","iopub.status.idle":"2023-09-24T09:55:38.832046Z","shell.execute_reply.started":"2023-09-24T09:55:38.801417Z","shell.execute_reply":"2023-09-24T09:55:38.830693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:38.834100Z","iopub.execute_input":"2023-09-24T09:55:38.834744Z","iopub.status.idle":"2023-09-24T09:55:38.859945Z","shell.execute_reply.started":"2023-09-24T09:55:38.834701Z","shell.execute_reply":"2023-09-24T09:55:38.858640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"LoanAmount\"]=df[\"LoanAmount\"].astype(int)\ndf[\"Credit_History\"]=df[\"Credit_History\"].astype(int)\ndf[\"Loan_Amount_Term\"]=df[\"Loan_Amount_Term\"].astype(int)\ndf[\"CoapplicantIncome\"]=df[\"CoapplicantIncome\"].astype(int)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:38.861416Z","iopub.execute_input":"2023-09-24T09:55:38.861952Z","iopub.status.idle":"2023-09-24T09:55:38.872946Z","shell.execute_reply.started":"2023-09-24T09:55:38.861920Z","shell.execute_reply":"2023-09-24T09:55:38.872042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `df.info()`: This code displays concise information about the DataFrame `df`. It includes details such as the number of non-null values, data types of columns, and memory usage. It's a useful way to get an overview of the DataFrame's structure.\n\n- `df[\"LoanAmount\"] = df[\"LoanAmount\"].astype(int)`: This code converts the data type of the \"LoanAmount\" column in the DataFrame `df` to integers. It is useful when you want to work with the \"LoanAmount\" values as whole numbers.\n","metadata":{}},{"cell_type":"code","source":"df.info() #checking","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:38.874109Z","iopub.execute_input":"2023-09-24T09:55:38.874590Z","iopub.status.idle":"2023-09-24T09:55:38.895084Z","shell.execute_reply.started":"2023-09-24T09:55:38.874560Z","shell.execute_reply":"2023-09-24T09:55:38.894242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Calculating Total income ","metadata":{}},{"cell_type":"code","source":"df[\"Total_Income\"] = df[\"ApplicantIncome\"] + df[\"CoapplicantIncome\"]","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:38.896363Z","iopub.execute_input":"2023-09-24T09:55:38.897263Z","iopub.status.idle":"2023-09-24T09:55:38.903130Z","shell.execute_reply.started":"2023-09-24T09:55:38.897229Z","shell.execute_reply":"2023-09-24T09:55:38.902313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Total_Income\"].mean() #average total income","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:38.904380Z","iopub.execute_input":"2023-09-24T09:55:38.905314Z","iopub.status.idle":"2023-09-24T09:55:38.919919Z","shell.execute_reply.started":"2023-09-24T09:55:38.905271Z","shell.execute_reply":"2023-09-24T09:55:38.918604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Loan_Term_years'] = (df['Loan_Amount_Term']/12).astype(int)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:38.921433Z","iopub.execute_input":"2023-09-24T09:55:38.922791Z","iopub.status.idle":"2023-09-24T09:55:38.935768Z","shell.execute_reply.started":"2023-09-24T09:55:38.922734Z","shell.execute_reply":"2023-09-24T09:55:38.934694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> I have computed the total income by aggregating both the applicant's income and co-applicant's income. This approach simplifies the comparison of the combined income with the individual incomes, making it more efficient for our predictive modeling process and reduced the Loan Amount terms months to years","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:38.937527Z","iopub.execute_input":"2023-09-24T09:55:38.938301Z","iopub.status.idle":"2023-09-24T09:55:38.963841Z","shell.execute_reply.started":"2023-09-24T09:55:38.938257Z","shell.execute_reply":"2023-09-24T09:55:38.962702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[[#'Loan_ID', \n        'Gender', 'Married', 'Dependents', 'Education',\n       'Self_Employed', 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',\n       #'Loan_Amount_Term',\n    'Credit_History', 'Property_Area', 'Total_Income', 'Loan_Term_years', 'Loan_Status']].copy()\n\n#removed the unwanted columns and reordered them ","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:38.965445Z","iopub.execute_input":"2023-09-24T09:55:38.966424Z","iopub.status.idle":"2023-09-24T09:55:38.979019Z","shell.execute_reply.started":"2023-09-24T09:55:38.966389Z","shell.execute_reply":"2023-09-24T09:55:38.978052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head() #checking","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:38.980380Z","iopub.execute_input":"2023-09-24T09:55:38.980860Z","iopub.status.idle":"2023-09-24T09:55:39.005984Z","shell.execute_reply.started":"2023-09-24T09:55:38.980828Z","shell.execute_reply":"2023-09-24T09:55:39.005175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Understanding","metadata":{}},{"cell_type":"code","source":"Features = ['Gender','Married','Dependents','Education','Self_Employed','Credit_History','Property_Area','Loan_Term_years']\nfor i in Features:\n    print(df[i].value_counts(normalize=True))\n    print(\"---------------------------------\")\n","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:39.007006Z","iopub.execute_input":"2023-09-24T09:55:39.007715Z","iopub.status.idle":"2023-09-24T09:55:39.024203Z","shell.execute_reply.started":"2023-09-24T09:55:39.007670Z","shell.execute_reply":"2023-09-24T09:55:39.023272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `Features = ['Gender','Married','Dependents','Education','Self_Employed','Credit_History','Property_Area','Loan_Term_years']`: This line defines a list named `Features` containing the names of *catagorical columns* (features) from the DataFrame that you want to analyze and print value counts for.\n\n  - `print(df[i].value_counts(normalize=True))`: Within the loop, this line calculates and prints the value counts for the current feature (`i`) in the DataFrame `df`. The `normalize=True` parameter normalizes the counts to represent proportions (percentages) rather than raw counts, making it easier to compare the distribution of each feature.\n\n  - `print(\"---------------------------------\")`: After printing the value counts for the current feature, this line adds a separator (a line of dashes) to visually distinguish the results for different features.\n\n\n> It's evident that the majority of the categorical features exhibit significant class imbalance. This observation highlights the unequal distribution of categories within these features, which can have implications for modeling and analysis\n","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\nsns.histplot(data=df,ax=axes[0], x=\"LoanAmount\", kde=True)\naxes[0].set_xlabel('Loan Amount')\naxes[0].set_title('Histogram and KDE Plot For LoanAmount')\n\nsns.histplot(data=df, ax=axes[1], x=\"Total_Income\", kde=True)\naxes[1].set_xlabel('Total_Income')\naxes[1].set_title('Histogram and KDE Plot For Total_Income')","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:39.029934Z","iopub.execute_input":"2023-09-24T09:55:39.030902Z","iopub.status.idle":"2023-09-24T09:55:39.910738Z","shell.execute_reply.started":"2023-09-24T09:55:39.030851Z","shell.execute_reply":"2023-09-24T09:55:39.909692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `fig, axes = plt.subplots(1, 2, figsize=(12, 4))`: This line creates a figure (`fig`) with two subplots (`axes`) arranged side by side in a single row. The `figsize` parameter specifies the dimensions of the figure.\n\n- `sns.histplot(data=df, ax=axes[0], x=\"LoanAmount\", kde=True)`: In the first subplot (`axes[0]`), this code uses Seaborn's `histplot` function to create a histogram and kernel density estimate (KDE) plot for the \"LoanAmount\" column from the DataFrame `df`. It visualizes the distribution of loan amounts with a histogram and overlays a KDE curve. The `x` parameter specifies the data to be plotted, and `kde=True` adds the KDE curve.\n\n- `axes[0].set_xlabel('Loan Amount')` and `axes[0].set_title('Histogram and KDE Plot For LoanAmount')`: These lines set the x-axis label and subplot title for the first plot, providing clarity and context.\n\n- `sns.histplot(data=df, ax=axes[1], x=\"Total_Income\", kde=True)`: In the second subplot (`axes[1]`), this code creates a similar histogram and KDE plot, but this time for the \"Total_Income\" column from the DataFrame `df`.\n\n- `axes[1].set_xlabel('Total_Income')` and `axes[1].set_title('Histogram and KDE Plot For Total_Income')`: These lines set the x-axis label and subplot title for the second plot, providing clear labeling and context.\n\n> It's evident that both the loan amount and total income distributions exhibit a right-skewed pattern. In these distributions, the mean is greater than the median, and the median is greater than the mode. This skewness suggests that a significant proportion of observations have higher values, causing the rightward tail in the distributions\n","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1,2, figsize=(12,4))\n\nsns.boxplot(data = df,ax=axes[0], x=\"LoanAmount\")\naxes[0].set_title('BoxPlot LoanAmount')\n\nsns.boxplot(data = df,ax=axes[1], x=\"Total_Income\")\naxes[1].set_title('Boxplot Total_Income')","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:39.912159Z","iopub.execute_input":"2023-09-24T09:55:39.912517Z","iopub.status.idle":"2023-09-24T09:55:40.235645Z","shell.execute_reply.started":"2023-09-24T09:55:39.912487Z","shell.execute_reply":"2023-09-24T09:55:40.234394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `sns.boxplot(data=df, ax=axes[0], x=\"LoanAmount\")`: In the first subplot (`axes[0]`), this code uses Seaborn's `boxplot` function to create a boxplot for the \"LoanAmount\" column from the DataFrame `df`. A boxplot is a graphical representation that shows the distribution of data and identifies potential outliers.\n\n- `sns.boxplot(data=df, ax=axes[1], x=\"Total_Income\")`: In the second subplot (`axes[1]`), this code creates a similar boxplot, but this time for the \"Total_Income\" column from the DataFrame `df`.\n\n- `axes[1].set_title('Boxplot Total_Income')`: This line sets the title for the second subplot, describing the content of the plot.\n\n> From these box plots, it's evident that both 'LoanAmount' and 'Total_Income' exhibit a considerable number of outliers. These outliers are data points that fall significantly beyond the whiskers of the box plots, indicating the presence of extreme values in these variables","metadata":{}},{"cell_type":"code","source":"df['Total_Income'].max()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:40.237173Z","iopub.execute_input":"2023-09-24T09:55:40.237598Z","iopub.status.idle":"2023-09-24T09:55:40.244886Z","shell.execute_reply.started":"2023-09-24T09:55:40.237556Z","shell.execute_reply":"2023-09-24T09:55:40.243689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[df['Total_Income'] != 81000]\n","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:40.246445Z","iopub.execute_input":"2023-09-24T09:55:40.246832Z","iopub.status.idle":"2023-09-24T09:55:40.258070Z","shell.execute_reply.started":"2023-09-24T09:55:40.246801Z","shell.execute_reply":"2023-09-24T09:55:40.256954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> I noticed that there was a particularly high outlier at $80,000 in the 'Total_Income' variable. As it appeared to be an extreme value, I decided to remove only that specific outlier from the dataset","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows = 2,ncols =2, figsize=(12,4)) # constrained_layout=True, used this for overlapping\nplt.subplots_adjust( hspace=0.8)\n\nsns.boxplot(data = df,ax=axes[0,0], x=\"LoanAmount\")\naxes[0,0].set_title('BoxPlot LoanAmount')\n\nsns.boxplot(data = df,ax=axes[0,1], x=\"Total_Income\")\naxes[0,1].set_title('Boxplot Total_Income')\n\nsns.violinplot(data=df, ax = axes[1,0], x =\"LoanAmount\")\naxes[1,0].set_title('ViolinPlot LoanAmount')\n\nsns.violinplot(data=df, ax = axes[1,1], x =\"Total_Income\")\naxes[1,1].set_title('ViolinPlot LoanAmount')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:40.259346Z","iopub.execute_input":"2023-09-24T09:55:40.260208Z","iopub.status.idle":"2023-09-24T09:55:41.026038Z","shell.execute_reply.started":"2023-09-24T09:55:40.260175Z","shell.execute_reply":"2023-09-24T09:55:41.024997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 4))`: This line creates a 2x2 grid of subplots (`axes`) within a single figure (`fig`) with a specified size. The grid layout allows for four plots to be displayed in a structured manner.\n\n- `plt.subplots_adjust(hspace=0.8)`: This line adjusts the vertical spacing (`hspace`) between the subplots, ensuring sufficient space between them for clarity.\n\n- `axes[0, 1].set_title('Boxplot Total_Income')`: This line sets the title for the top-right subplot.\n\n- `sns.violinplot(data=df, ax=axes[1, 0], x=\"LoanAmount\")`: In the bottom-left subplot (`axes[1, 0]`), this code uses Seaborn's `violinplot` function to create a violin plot for the \"LoanAmount\" column, providing insights into the distribution and density of data points.\n\n- `axes[1, 0].set_title('ViolinPlot LoanAmount')`: This line sets the title for the bottom-left subplot.\n\n- `sns.violinplot(data=df, ax=axes[1, 1], x=\"Total_Income\")`: In the bottom-right subplot (`axes[1, 1]`), a similar violin plot is created, but this time for the \"Total_Income\" column from the DataFrame `df`.\n\n- `axes[1, 1].set_title('ViolinPlot LoanAmount')`: This line sets the title for the bottom-right subplot.\n\n","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(data=df, x=\"LoanAmount\", y=\"Total_Income\", hue=\"Loan_Status\")","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:41.027415Z","iopub.execute_input":"2023-09-24T09:55:41.028675Z","iopub.status.idle":"2023-09-24T09:55:41.361387Z","shell.execute_reply.started":"2023-09-24T09:55:41.028631Z","shell.execute_reply":"2023-09-24T09:55:41.360115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_list = [\"Gender\",\"Married\",\"Education\",\"Dependents\", \"Self_Employed\",\"Credit_History\",\"Property_Area\",\"Loan_Term_years\"]\ntotal_plots = len(data_list)\n\nnrows, ncols = 4, 2\n\n\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 16)) \nfor i in range(total_plots):\n    row = i // ncols\n    col = i % ncols\n    ax = axes[row, col]\n\n    sns.scatterplot(data=df, x=\"LoanAmount\", y=\"Total_Income\",ax=ax, hue=data_list[i])\n\n    ax.set_title(f'Plot {i+1}')\n    ax.set_xlabel('Loan Amount')\n    ax.set_ylabel('Total Income')\n\n# Adjust spacing between subplots\nplt.tight_layout()\n\n# Display the plots\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:41.362516Z","iopub.execute_input":"2023-09-24T09:55:41.362861Z","iopub.status.idle":"2023-09-24T09:55:43.974260Z","shell.execute_reply.started":"2023-09-24T09:55:41.362833Z","shell.execute_reply":"2023-09-24T09:55:43.972945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `data_list`: This list contains the names of the categorical variables we want to analyze with scatterplots.\n\n- `total_plots`: This variable stores the total number of plots to be created, which is determined by the length of `data_list`.\n\n- `nrows, ncols`: These variables specify the number of rows and columns for the subplot grid.\n\n- `fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 16))`: This line creates a grid of subplots (`axes`) with the specified number of rows and columns. The `figsize` parameter determines the overall size of the figure.\n\n- The `for` loop iterates through the categorical variables in `data_list`:\n  - `row` and `col` are calculated to determine the current position in the subplot grid.\n  - `ax` represents the current subplot where the scatterplot will be drawn.\n  - `sns.scatterplot()` is used to create a scatterplot of \"LoanAmount\" against \"Total_Income,\" with points colored by the categorical variable specified in `data_list[i]`.\n  - `ax.set_title()`, `ax.set_xlabel()`, and `ax.set_ylabel()` set the title, x-axis label, and y-axis label for each subplot.\n  \n- `plt.tight_layout()`: This line adjusts the spacing between subplots to ensure they are well-organized and do not overlap.\n\n- `plt.show()`: Finally, this command displays the scatterplot grid, showing how \"LoanAmount\" and \"Total_Income\" relate to each categorical variable in `data_list`.\n\n\n","metadata":{}},{"cell_type":"code","source":"sns.jointplot(data=df, x=\"LoanAmount\", y=\"Total_Income\", kind=\"hist\")","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:43.975901Z","iopub.execute_input":"2023-09-24T09:55:43.978316Z","iopub.status.idle":"2023-09-24T09:55:44.731934Z","shell.execute_reply.started":"2023-09-24T09:55:43.978275Z","shell.execute_reply":"2023-09-24T09:55:44.730671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.swarmplot(data=df, x=\"Loan_Status\", y=\"Total_Income\",  hue=\"Credit_History\")","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:44.733439Z","iopub.execute_input":"2023-09-24T09:55:44.733789Z","iopub.status.idle":"2023-09-24T09:55:47.724781Z","shell.execute_reply.started":"2023-09-24T09:55:44.733750Z","shell.execute_reply":"2023-09-24T09:55:47.722831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The joint plot displays the relationship between two variables, \"LoanAmount\" (on the x-axis) and \"Total_Income\" (on the y-axis), with the chosen kind of plot being a histogram\n\n- `sns.swarmplot(data=df, x=\"Loan_Status\", y=\"Total_Income\",  hue=\"Credit_History\")`: This line creates a swarm plot using Seaborn.\n\nA swarm plot is a categorical scatter plot that displays individual data points along a categorical axis, in this case, \"Loan_Status\" on the x-axis and \"Total_Income\" on the y-axis. Additionally, it uses the \"Credit_History\" variable to color-code the data points, providing information about a third categorical variable.\n\nA swarm plot is useful for visualizing the distribution of data points within different categories and understanding the relationships between multiple variables. In this case, it can help identify any patterns or trends related to loan status, total income, and credit history.\n\n> In the swarm plot visualization, a clear pattern emerges: individuals who were approved for a loan typically have a documented credit history, while those whose loan applications were not approved often lack a credit history. This visual representation underscores the significant influence of credit history on loan approval outcomes.","metadata":{}},{"cell_type":"code","source":"df1 = df[[#'Loan_ID', \n        'Gender', 'Married', 'Dependents', 'Education',\n        'Self_Employed', #'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',\n        #'Loan_Amount_Term','Credit_History','Total_Income', \n        'Property_Area',  'Loan_Term_years', 'Loan_Status']].copy()\n\nfor col in df1.columns:\n    print(f'{col}: {df1[col].unique()}')\n    print(\" \") #new line","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:47.726454Z","iopub.execute_input":"2023-09-24T09:55:47.726876Z","iopub.status.idle":"2023-09-24T09:55:47.736852Z","shell.execute_reply.started":"2023-09-24T09:55:47.726845Z","shell.execute_reply":"2023-09-24T09:55:47.735587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"  - `print(f'{col}: {df1[col].unique()}')`: It prints the name of the column followed by the unique values present in that column. This provides insight into the distinct values within each selected column.\n","metadata":{}},{"cell_type":"code","source":"df['Gender'] = df['Gender'].replace({'Male': 1, 'Female': 0})\ndf['Married'] = df['Married'].replace({'Yes': 1, 'No': 0})\ndf['Dependents'] = df['Dependents'].replace({'1': 1, '0': 0, '2': 2, '3+' : 3})\ndf['Education'] = df['Education'].replace({'Graduate': 1, 'Not Graduate': 0})\ndf['Self_Employed'] = df['Self_Employed'].replace({'No': 1, 'Yes': 0})\ndf['Property_Area'] = df['Property_Area'].replace({'Rural': 1, 'Urban': 0, 'Semiurban': 2})\ndf['Loan_Status'] = df['Loan_Status'].replace({'N': 1, 'Y': 0})\n","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:47.738445Z","iopub.execute_input":"2023-09-24T09:55:47.738811Z","iopub.status.idle":"2023-09-24T09:55:47.757222Z","shell.execute_reply.started":"2023-09-24T09:55:47.738779Z","shell.execute_reply":"2023-09-24T09:55:47.756009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> The provided code is performing data preprocessing tasks by converting categorical variables into numerical format using the `replace` method in pandas. \n","metadata":{}},{"cell_type":"code","source":"df = df[[#'Loan_ID', \n        'Gender', 'Married', 'Dependents', 'Education',\n       'Self_Employed', #'ApplicantIncome', 'CoapplicantIncome', \n        'LoanAmount',\n       #'Loan_Amount_Term',\n    'Credit_History', 'Property_Area', 'Total_Income', 'Loan_Term_years', 'Loan_Status']].copy()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:47.760739Z","iopub.execute_input":"2023-09-24T09:55:47.761185Z","iopub.status.idle":"2023-09-24T09:55:47.774080Z","shell.execute_reply.started":"2023-09-24T09:55:47.761144Z","shell.execute_reply":"2023-09-24T09:55:47.773071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(10) #check the change","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:47.775426Z","iopub.execute_input":"2023-09-24T09:55:47.775771Z","iopub.status.idle":"2023-09-24T09:55:47.796733Z","shell.execute_reply.started":"2023-09-24T09:55:47.775742Z","shell.execute_reply":"2023-09-24T09:55:47.795386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info() #Checking whether everything has changed int","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:47.798218Z","iopub.execute_input":"2023-09-24T09:55:47.798582Z","iopub.status.idle":"2023-09-24T09:55:47.815922Z","shell.execute_reply.started":"2023-09-24T09:55:47.798551Z","shell.execute_reply":"2023-09-24T09:55:47.814696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.drop('Loan_Status',axis = 1)\nY = df['Loan_Status']","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:47.817176Z","iopub.execute_input":"2023-09-24T09:55:47.817998Z","iopub.status.idle":"2023-09-24T09:55:47.836747Z","shell.execute_reply.started":"2023-09-24T09:55:47.817964Z","shell.execute_reply":"2023-09-24T09:55:47.835314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ros = RandomOverSampler()\nX, Y = ros.fit_resample(X, Y)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:47.838538Z","iopub.execute_input":"2023-09-24T09:55:47.838891Z","iopub.status.idle":"2023-09-24T09:55:47.856605Z","shell.execute_reply.started":"2023-09-24T09:55:47.838861Z","shell.execute_reply":"2023-09-24T09:55:47.855114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `X = df.drop('Loan_Status', axis=1)`: In this line, a new DataFrame `X` is created by dropping the 'Loan_Status' column from the original DataFrame `df`. This step separates the features (independent variables) from the target variable ('Loan_Status').\n\n- `Y = df['Loan_Status']`: Here, a new Series `Y` is created, containing only the 'Loan_Status' column from the original DataFrame `df`. This variable represents the target or dependent variable that we want to predict.\n\n- `ros = RandomOverSampler()`: An instance of the `RandomOverSampler` class is created. This class is part of the imbalanced-learn library and is used to address class imbalance by oversampling the minority class.\n\n- `X, Y = ros.fit_resample(X, Y)`: This is where the resampling takes place. The `fit_resample` method of the `RandomOverSampler` class is used to balance the class distribution. It oversamples the minority class (where 'Loan_Status' is 1) by generating synthetic samples to match the number of samples in the majority class (where 'Loan_Status' is 0). As a result, both classes are balanced in terms of the number of samples.\n\n > The resampling technique is essential when dealing with imbalanced datasets, as it helps improve the performance of machine learning models by ensuring that both classes have an adequate number of samples for training. This can lead to better model generalization and prediction accuracy, especially when the dataset has a significant class imbalance.\n","metadata":{}},{"cell_type":"code","source":"sns.countplot(x=Y) #checking after balancing","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:47.857712Z","iopub.execute_input":"2023-09-24T09:55:47.858075Z","iopub.status.idle":"2023-09-24T09:55:48.092197Z","shell.execute_reply.started":"2023-09-24T09:55:47.858043Z","shell.execute_reply":"2023-09-24T09:55:48.091229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ML Modelling ","metadata":{}},{"cell_type":"code","source":"X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, shuffle=True)\n\nprint(\"Size of X_train DataFrame: \", X_train.shape)\nprint(\"Size of X_val DataFrame: \", X_val.shape)\nprint(\"Size of Y_train DataFrame: \", Y_train.shape)\nprint(\"Size of Y_val DataFrame: \", Y_val.shape)\n\n#splitting the current X and Y into training and test dataset ","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:48.093427Z","iopub.execute_input":"2023-09-24T09:55:48.093941Z","iopub.status.idle":"2023-09-24T09:55:48.101325Z","shell.execute_reply.started":"2023-09-24T09:55:48.093911Z","shell.execute_reply":"2023-09-24T09:55:48.100477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n\ndef show_scores(model):\n    train_preds = model.predict(X_train)\n    val_preds = model.predict(X_val)\n    scores = {\"Training F1 Score\": f1_score(Y_train, train_preds),\n              \"Validation F1 Score\": f1_score(Y_val, val_preds),\n              \"Training Precision Score\": precision_score(Y_train, train_preds),\n              \"Validation Precision Score\": precision_score(Y_val, val_preds),\n              \"Training Recall Score\": recall_score(Y_train, train_preds),\n              \"Validation Recall Score\": recall_score(Y_val, val_preds),\n              \"Training Accuracy Score\": accuracy_score(Y_train, train_preds),\n              \"Validation Accuracy Score\": accuracy_score(Y_val, val_preds),\n              \"Training Confusion Matrix\": confusion_matrix(Y_train, train_preds),\n              \"Validation Confusion Matrix\": confusion_matrix(Y_val, val_preds)}\n\n    # Iterate through the dictionary and print each score with a newline character\n    for score_name, score_value in scores.items():\n        print(f\"{score_name}: {score_value}\\n\")\n\n    #return scores\n","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:48.102660Z","iopub.execute_input":"2023-09-24T09:55:48.102976Z","iopub.status.idle":"2023-09-24T09:55:48.116537Z","shell.execute_reply.started":"2023-09-24T09:55:48.102948Z","shell.execute_reply":"2023-09-24T09:55:48.115702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `show_scores` function is designed to assess the performance of a machine learning model by calculating several classification metrics for both the training and validation datasets. Here's a summary of what the code does:\n\n- `train_preds = model.predict(X_train)`: This line generates predictions for the training dataset (`X_train`) using the provided machine learning model (`model`).\n\n- `val_preds = model.predict(X_val)`: Similarly, this line generates predictions for the validation dataset (`X_val`) using the same model.\n\n- `scores = {...}`: In this dictionary, various classification metrics are computed and stored. These metrics include F1 Score, Precision Score, Recall Score, Accuracy Score, and Confusion Matrix for both the training and validation datasets.\n\n- The metrics are calculated using functions from the `sklearn.metrics` module, such as `f1_score`, `precision_score`, `recall_score`, `accuracy_score`, and `confusion_matrix`. These metrics are common tools for evaluating the performance of classification models.\n\n- The function then iterates through the `scores` dictionary and prints each metric along with its corresponding value. It adds a newline character (`\\n`) after printing each metric to separate them.\n\n\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier()\nrf_clf.fit(X_train, Y_train)\n\nprint(\"Baseline Random Forest model scores\\n\")\nshow_scores(rf_clf)\n# print(\"Baseline Random Forest model scores\")","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:48.117680Z","iopub.execute_input":"2023-09-24T09:55:48.118216Z","iopub.status.idle":"2023-09-24T09:55:48.378150Z","shell.execute_reply.started":"2023-09-24T09:55:48.118184Z","shell.execute_reply":"2023-09-24T09:55:48.377128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\n\nxt_clf = ExtraTreesClassifier()\nxt_clf.fit(X_train, Y_train)\n\nprint(\"Baseline Extra Trees Classifier scores\\n\")\nshow_scores(xt_clf)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:48.379343Z","iopub.execute_input":"2023-09-24T09:55:48.379656Z","iopub.status.idle":"2023-09-24T09:55:48.591458Z","shell.execute_reply.started":"2023-09-24T09:55:48.379605Z","shell.execute_reply":"2023-09-24T09:55:48.590196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\nestimators = []\nestimators.append(('RF', RandomForestClassifier()))\nestimators.append(('XT', ExtraTreesClassifier()))\n\nvt_clf = VotingClassifier(estimators = estimators, voting='soft', verbose = True)\nvt_clf.fit(X_train, Y_train)\n\n\nprint(\"Baseline Voting Classifier scores\\n\")\nshow_scores(vt_clf)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:48.592851Z","iopub.execute_input":"2023-09-24T09:55:48.593812Z","iopub.status.idle":"2023-09-24T09:55:49.054361Z","shell.execute_reply.started":"2023-09-24T09:55:48.593770Z","shell.execute_reply":"2023-09-24T09:55:49.053162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GridSearchCV","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Define your Random Forest classifier\nrf_clf = RandomForestClassifier()\n\n# Define a dictionary of hyperparameter values to search\nparam_grid = {\n    'n_estimators': [100, 200],            # Number of trees in the forest\n    'max_depth': [None, 10, 20, 30],            # Maximum depth of each tree\n    'min_samples_split': [2, 5],           # Minimum samples required to split a node\n    'min_samples_leaf': [1, 2],             # Minimum samples required in a leaf node\n    'max_features': ['auto', 'sqrt'],          # Number of features to consider for the best split\n    'bootstrap': [True],                # Whether to bootstrap samples when building trees\n    'criterion': ['gini', 'entropy']          \n}\n\n# Create GridSearchCV instance\nrf_grid_search = GridSearchCV(estimator=rf_clf, param_grid=param_grid, \n                           cv=5, scoring='accuracy', verbose=2, n_jobs=-1)\n\n# Fit the GridSearchCV to your training data\nrf_grid_search.fit(X_train, Y_train)\n\n# Print the best hyperparameters found\nbest_params = rf_grid_search.best_params_\nprint(\"Best Hyperparameters:\")\nprint(best_params)\n\n# Get the best model from the grid search\nbest_rf_model = rf_grid_search.best_estimator_\n\nshow_scores(best_rf_model)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:55:49.055951Z","iopub.execute_input":"2023-09-24T09:55:49.056671Z","iopub.status.idle":"2023-09-24T09:56:30.117144Z","shell.execute_reply.started":"2023-09-24T09:55:49.056635Z","shell.execute_reply":"2023-09-24T09:56:30.115278Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier(bootstrap = True, criterion= 'entropy', max_depth= 30, max_features= 'sqrt', min_samples_leaf= 1, min_samples_split= 2, n_estimators= 200)\nrf_clf.fit(X_train, Y_train)\n\nprint(\"Baseline Random Forest model scores\\n\")\nshow_scores(rf_clf)\n# print(\"Baseline Random Forest model scores\")","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:56:30.118794Z","iopub.execute_input":"2023-09-24T09:56:30.119379Z","iopub.status.idle":"2023-09-24T09:56:30.614183Z","shell.execute_reply.started":"2023-09-24T09:56:30.119336Z","shell.execute_reply":"2023-09-24T09:56:30.613007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These hyperparameters are tailored for the Random Forest model to achieve specific performance goals. After configuring the classifier, it is trained on the provided training data (`X_train` and `Y_train`).\n\n> It's worth noting that since I haven't fixed a random state, there might be slight variations in results across different runs. However, the model I'm presenting here represents one of the most efficient and effective variations I've identified during my GridSearchCV optimization efforts. \n","metadata":{}},{"cell_type":"code","source":"#uncomment this if you the above model in a pickle file\n\n# import pickle\n# filename = 'model.pkl'\n# pickle.dump(rf_clf, open(filename, 'wb'))","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:56:30.615581Z","iopub.execute_input":"2023-09-24T09:56:30.616005Z","iopub.status.idle":"2023-09-24T09:56:30.620669Z","shell.execute_reply.started":"2023-09-24T09:56:30.615965Z","shell.execute_reply":"2023-09-24T09:56:30.619534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Importances","metadata":{}},{"cell_type":"code","source":"feature_importances = rf_clf.feature_importances_","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:56:30.622339Z","iopub.execute_input":"2023-09-24T09:56:30.622897Z","iopub.status.idle":"2023-09-24T09:56:30.648999Z","shell.execute_reply.started":"2023-09-24T09:56:30.622857Z","shell.execute_reply":"2023-09-24T09:56:30.646701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(feature_importances)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:56:30.650960Z","iopub.execute_input":"2023-09-24T09:56:30.651868Z","iopub.status.idle":"2023-09-24T09:56:30.658850Z","shell.execute_reply.started":"2023-09-24T09:56:30.651821Z","shell.execute_reply":"2023-09-24T09:56:30.657707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nplt.bar(X.columns, feature_importances)\nplt.xlabel('Feature')\nplt.ylabel('Importance')\nplt.title('Random Forest Feature Importance')\nplt.xticks(rotation=40)\nplt.tight_layout()\nplt.show()\n\n#plotting the model's Feature importance","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:56:30.661074Z","iopub.execute_input":"2023-09-24T09:56:30.661566Z","iopub.status.idle":"2023-09-24T09:56:31.167832Z","shell.execute_reply.started":"2023-09-24T09:56:30.661529Z","shell.execute_reply":"2023-09-24T09:56:31.166630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> The feature importances plot reveals that within this model, certain features hold substantial significance. Specifically, it highlights the pivotal roles played by \"Total Income,\" \"Loan Amount,\" and \"Credit History.\" These features are instrumental in shaping the model's predictions, signifying their critical importance in assessing loan approval outcomes.\n\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"In conclusion, the developed machine learning model represents a valuable tool for predicting loan approval outcomes. Through a comprehensive analysis of various input parameters such as gender, marital status, education, income, and credit history, the model has demonstrated its capability to make informed decisions regarding loan eligibility.\n\n","metadata":{}},{"cell_type":"markdown","source":"## Areas for Improvement:\n\n1. **Detailed Exploratory Data Analysis (EDA):** To enhance the model's performance, conducting a more comprehensive EDA is crucial. This includes exploring data distributions, correlations, and relationships between variables in greater detail.\n\n2. **Incorporate Additional Data Insights:** Seek to extract deeper insights from the data, such as identifying patterns or trends that might be crucial in predicting loan approval outcomes. Utilize data visualization techniques and statistical analysis to uncover hidden information.\n\n3. **Data Transformation for Skewed Data:** Addressing right-skewed data distributions can improve model accuracy. Consider applying techniques like log transformations to make the data more symmetrical and suitable for modeling.\n\n4. **Outlier Handling with Winsorization:** Implement Winsorization to manage outliers in the dataset. This technique caps extreme values to minimize their impact on the model's performance while preserving valuable information.\n\n5. **Imputation Methods:** Evaluate different imputation methods, such as mean or median imputation, for handling missing data effectively. Select the method that aligns best with the dataset's characteristics.\n\n6. **Experiment with Various Models:** Explore a range of machine learning models beyond the Random Forest Classifier used in the initial model. Evaluate models like Gradient Boosting, Support Vector Machines, or Neural Networks to identify which one performs optimally for loan approval prediction.\n\nThese improvements aim to create a more robust and accurate predictive model for loan approval. By enhancing data exploration, addressing data quality issues, and experimenting with different modeling approaches, the model's performance can be significantly enhanced.\n","metadata":{}},{"cell_type":"markdown","source":"Github : https://github.com/kinba09/Home_Loan_Approval | Streamlit : will update | API : will update ","metadata":{}},{"cell_type":"markdown","source":"<div style=\"text-align: center;\">\n  <h1>Do comment and Upvote if you like my work</h1>\n</div>\n","metadata":{}}]}